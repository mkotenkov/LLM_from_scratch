{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизатор\n",
    "\n",
    "Токенизатор основан на алгоритме BPE. \\\n",
    "Дополнительно для более качественной токенизации применяется несколько ограничений на слияние токенов. \\\n",
    "\\\n",
    "Запрещено слияние:\n",
    "- для объединения в один токен нескольких слов (при этом, токены из нескольких пробелов допускаются) \n",
    "- если один токен - буквенный, другой - нет. Несколько примеров:\n",
    "  - \" help\" + \"ful\" - можно\n",
    "  - \" help\" + \"ful.\" - нельзя\n",
    "  - \"..\" + \".\" - можно\n",
    "- если один токен - цифровой, другой - нет. (запрещает токены вроде \"2021.\", \"2021,\")\n",
    "- если один из токенов - символ новой строки (запрещает токены вроде \"\\nExternal\")\n",
    "  \n",
    "\\\n",
    "Я обучил два токенизатора:\n",
    "1. Учитывает регистр. 25000 слияний. Словарь: `checkpoints/tokenizer/tokenizer_25k_10k.vocab`\n",
    "2. Не учитывает регистр. 15000 слияний. Словарь: `checkpoints/tokenizer/tokenizer_15k_10k_uncased.vocab`\n",
    "\n",
    "Оба обучены на 10000 статей из вики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "What is a piece of text?\n",
    "A text is a passage of words that conveys a set of meanings to the person who is reading it. \n",
    "It’s a body of written work, in various forms and structures, that can be words, phrases and sentences that piece together a passage of written work.\n",
    "To put it as simply as possible, it is a group of words. But it can come in many different forms.\n",
    "A text can be written materials, such as books, magazines, newspapers, or online content. \n",
    "But it can also be other things, those that we may not associate with standard text. \n",
    "Text could be movies, scripts, paintings, songs, political cartoons, advertisements and maps. \n",
    "If we can look at something with words and sentences, explore it, find layers of meaning in it, and draw information and conclusions from it, you’re looking at a text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cased tokenization:\n",
      "\u001b[41m\n",
      "\u001b[0m\u001b[44mWhat\u001b[0m\u001b[41m is\u001b[0m\u001b[44m a\u001b[0m\u001b[41m piece\u001b[0m\u001b[44m of\u001b[0m\u001b[41m text\u001b[0m\u001b[44m?\u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mA\u001b[0m\u001b[41m text\u001b[0m\u001b[44m is\u001b[0m\u001b[41m a\u001b[0m\u001b[44m passage\u001b[0m\u001b[41m of\u001b[0m\u001b[44m words\u001b[0m\u001b[41m that\u001b[0m\u001b[44m conve\u001b[0m\u001b[41mys\u001b[0m\u001b[44m a\u001b[0m\u001b[41m set\u001b[0m\u001b[44m of\u001b[0m\u001b[41m mean\u001b[0m\u001b[44mings\u001b[0m\u001b[41m to\u001b[0m\u001b[44m the\u001b[0m\u001b[41m person\u001b[0m\u001b[44m who\u001b[0m\u001b[41m is\u001b[0m\u001b[44m reading\u001b[0m\u001b[41m it\u001b[0m\u001b[44m.\u001b[0m\u001b[41m \u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41mIt\u001b[0m\u001b[44m’\u001b[0m\u001b[41ms\u001b[0m\u001b[44m a\u001b[0m\u001b[41m body\u001b[0m\u001b[44m of\u001b[0m\u001b[41m written\u001b[0m\u001b[44m work\u001b[0m\u001b[41m,\u001b[0m\u001b[44m in\u001b[0m\u001b[41m various\u001b[0m\u001b[44m forms\u001b[0m\u001b[41m and\u001b[0m\u001b[44m structures\u001b[0m\u001b[41m,\u001b[0m\u001b[44m that\u001b[0m\u001b[41m can\u001b[0m\u001b[44m be\u001b[0m\u001b[41m words\u001b[0m\u001b[44m,\u001b[0m\u001b[41m phrases\u001b[0m\u001b[44m and\u001b[0m\u001b[41m sentences\u001b[0m\u001b[44m that\u001b[0m\u001b[41m piece\u001b[0m\u001b[44m together\u001b[0m\u001b[41m a\u001b[0m\u001b[44m passage\u001b[0m\u001b[41m of\u001b[0m\u001b[44m written\u001b[0m\u001b[41m work\u001b[0m\u001b[44m.\u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mTo\u001b[0m\u001b[41m put\u001b[0m\u001b[44m it\u001b[0m\u001b[41m as\u001b[0m\u001b[44m simply\u001b[0m\u001b[41m as\u001b[0m\u001b[44m possible\u001b[0m\u001b[41m,\u001b[0m\u001b[44m it\u001b[0m\u001b[41m is\u001b[0m\u001b[44m a\u001b[0m\u001b[41m group\u001b[0m\u001b[44m of\u001b[0m\u001b[41m words\u001b[0m\u001b[44m.\u001b[0m\u001b[41m But\u001b[0m\u001b[44m it\u001b[0m\u001b[41m can\u001b[0m\u001b[44m come\u001b[0m\u001b[41m in\u001b[0m\u001b[44m many\u001b[0m\u001b[41m different\u001b[0m\u001b[44m forms\u001b[0m\u001b[41m.\u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41mA\u001b[0m\u001b[44m text\u001b[0m\u001b[41m can\u001b[0m\u001b[44m be\u001b[0m\u001b[41m written\u001b[0m\u001b[44m materials\u001b[0m\u001b[41m,\u001b[0m\u001b[44m such\u001b[0m\u001b[41m as\u001b[0m\u001b[44m books\u001b[0m\u001b[41m,\u001b[0m\u001b[44m magazines\u001b[0m\u001b[41m,\u001b[0m\u001b[44m newspapers\u001b[0m\u001b[41m,\u001b[0m\u001b[44m or\u001b[0m\u001b[41m online\u001b[0m\u001b[44m content\u001b[0m\u001b[41m.\u001b[0m\u001b[44m \u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mBut\u001b[0m\u001b[41m it\u001b[0m\u001b[44m can\u001b[0m\u001b[41m also\u001b[0m\u001b[44m be\u001b[0m\u001b[41m other\u001b[0m\u001b[44m things\u001b[0m\u001b[41m,\u001b[0m\u001b[44m those\u001b[0m\u001b[41m that\u001b[0m\u001b[44m we\u001b[0m\u001b[41m may\u001b[0m\u001b[44m not\u001b[0m\u001b[41m associate\u001b[0m\u001b[44m with\u001b[0m\u001b[41m standard\u001b[0m\u001b[44m text\u001b[0m\u001b[41m.\u001b[0m\u001b[44m \u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mT\u001b[0m\u001b[41mext\u001b[0m\u001b[44m could\u001b[0m\u001b[41m be\u001b[0m\u001b[44m movies\u001b[0m\u001b[41m,\u001b[0m\u001b[44m scripts\u001b[0m\u001b[41m,\u001b[0m\u001b[44m paintings\u001b[0m\u001b[41m,\u001b[0m\u001b[44m songs\u001b[0m\u001b[41m,\u001b[0m\u001b[44m political\u001b[0m\u001b[41m cartoons\u001b[0m\u001b[44m,\u001b[0m\u001b[41m advertisements\u001b[0m\u001b[44m and\u001b[0m\u001b[41m maps\u001b[0m\u001b[44m.\u001b[0m\u001b[41m \u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41mIf\u001b[0m\u001b[44m we\u001b[0m\u001b[41m can\u001b[0m\u001b[44m look\u001b[0m\u001b[41m at\u001b[0m\u001b[44m something\u001b[0m\u001b[41m with\u001b[0m\u001b[44m words\u001b[0m\u001b[41m and\u001b[0m\u001b[44m sentences\u001b[0m\u001b[41m,\u001b[0m\u001b[44m explore\u001b[0m\u001b[41m it\u001b[0m\u001b[44m,\u001b[0m\u001b[41m find\u001b[0m\u001b[44m layers\u001b[0m\u001b[41m of\u001b[0m\u001b[44m meaning\u001b[0m\u001b[41m in\u001b[0m\u001b[44m it\u001b[0m\u001b[41m,\u001b[0m\u001b[44m and\u001b[0m\u001b[41m draw\u001b[0m\u001b[44m information\u001b[0m\u001b[41m and\u001b[0m\u001b[44m con\u001b[0m\u001b[41mclusions\u001b[0m\u001b[44m from\u001b[0m\u001b[41m it\u001b[0m\u001b[44m,\u001b[0m\u001b[41m you\u001b[0m\u001b[44m’\u001b[0m\u001b[41mre\u001b[0m\u001b[44m looking\u001b[0m\u001b[41m at\u001b[0m\u001b[44m a\u001b[0m\u001b[41m text\u001b[0m\u001b[44m.\u001b[0m\n",
      "uncased tokenization:\n",
      "\u001b[41m\n",
      "\u001b[0m\u001b[44mwhat\u001b[0m\u001b[41m is\u001b[0m\u001b[44m a\u001b[0m\u001b[41m piece\u001b[0m\u001b[44m of\u001b[0m\u001b[41m text\u001b[0m\u001b[44m?\u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44ma\u001b[0m\u001b[41m text\u001b[0m\u001b[44m is\u001b[0m\u001b[41m a\u001b[0m\u001b[44m passage\u001b[0m\u001b[41m of\u001b[0m\u001b[44m words\u001b[0m\u001b[41m that\u001b[0m\u001b[44m convey\u001b[0m\u001b[41ms\u001b[0m\u001b[44m a\u001b[0m\u001b[41m set\u001b[0m\u001b[44m of\u001b[0m\u001b[41m mean\u001b[0m\u001b[44mings\u001b[0m\u001b[41m to\u001b[0m\u001b[44m the\u001b[0m\u001b[41m person\u001b[0m\u001b[44m who\u001b[0m\u001b[41m is\u001b[0m\u001b[44m reading\u001b[0m\u001b[41m it\u001b[0m\u001b[44m.\u001b[0m\u001b[41m \u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41mit\u001b[0m\u001b[44m’\u001b[0m\u001b[41ms\u001b[0m\u001b[44m a\u001b[0m\u001b[41m body\u001b[0m\u001b[44m of\u001b[0m\u001b[41m written\u001b[0m\u001b[44m work\u001b[0m\u001b[41m,\u001b[0m\u001b[44m in\u001b[0m\u001b[41m various\u001b[0m\u001b[44m forms\u001b[0m\u001b[41m and\u001b[0m\u001b[44m structures\u001b[0m\u001b[41m,\u001b[0m\u001b[44m that\u001b[0m\u001b[41m can\u001b[0m\u001b[44m be\u001b[0m\u001b[41m words\u001b[0m\u001b[44m,\u001b[0m\u001b[41m phr\u001b[0m\u001b[44mases\u001b[0m\u001b[41m and\u001b[0m\u001b[44m sent\u001b[0m\u001b[41mences\u001b[0m\u001b[44m that\u001b[0m\u001b[41m piece\u001b[0m\u001b[44m together\u001b[0m\u001b[41m a\u001b[0m\u001b[44m passage\u001b[0m\u001b[41m of\u001b[0m\u001b[44m written\u001b[0m\u001b[41m work\u001b[0m\u001b[44m.\u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mto\u001b[0m\u001b[41m put\u001b[0m\u001b[44m it\u001b[0m\u001b[41m as\u001b[0m\u001b[44m simply\u001b[0m\u001b[41m as\u001b[0m\u001b[44m possible\u001b[0m\u001b[41m,\u001b[0m\u001b[44m it\u001b[0m\u001b[41m is\u001b[0m\u001b[44m a\u001b[0m\u001b[41m group\u001b[0m\u001b[44m of\u001b[0m\u001b[41m words\u001b[0m\u001b[44m.\u001b[0m\u001b[41m but\u001b[0m\u001b[44m it\u001b[0m\u001b[41m can\u001b[0m\u001b[44m come\u001b[0m\u001b[41m in\u001b[0m\u001b[44m many\u001b[0m\u001b[41m different\u001b[0m\u001b[44m forms\u001b[0m\u001b[41m.\u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41ma\u001b[0m\u001b[44m text\u001b[0m\u001b[41m can\u001b[0m\u001b[44m be\u001b[0m\u001b[41m written\u001b[0m\u001b[44m materials\u001b[0m\u001b[41m,\u001b[0m\u001b[44m such\u001b[0m\u001b[41m as\u001b[0m\u001b[44m books\u001b[0m\u001b[41m,\u001b[0m\u001b[44m magazines\u001b[0m\u001b[41m,\u001b[0m\u001b[44m newspapers\u001b[0m\u001b[41m,\u001b[0m\u001b[44m or\u001b[0m\u001b[41m online\u001b[0m\u001b[44m content\u001b[0m\u001b[41m.\u001b[0m\u001b[44m \u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mbut\u001b[0m\u001b[41m it\u001b[0m\u001b[44m can\u001b[0m\u001b[41m also\u001b[0m\u001b[44m be\u001b[0m\u001b[41m other\u001b[0m\u001b[44m things\u001b[0m\u001b[41m,\u001b[0m\u001b[44m those\u001b[0m\u001b[41m that\u001b[0m\u001b[44m we\u001b[0m\u001b[41m may\u001b[0m\u001b[44m not\u001b[0m\u001b[41m associate\u001b[0m\u001b[44m with\u001b[0m\u001b[41m standard\u001b[0m\u001b[44m text\u001b[0m\u001b[41m.\u001b[0m\u001b[44m \u001b[0m\u001b[41m\n",
      "\u001b[0m\u001b[44mtext\u001b[0m\u001b[41m could\u001b[0m\u001b[44m be\u001b[0m\u001b[41m movies\u001b[0m\u001b[44m,\u001b[0m\u001b[41m sc\u001b[0m\u001b[44mripts\u001b[0m\u001b[41m,\u001b[0m\u001b[44m paintings\u001b[0m\u001b[41m,\u001b[0m\u001b[44m songs\u001b[0m\u001b[41m,\u001b[0m\u001b[44m political\u001b[0m\u001b[41m cart\u001b[0m\u001b[44moons\u001b[0m\u001b[41m,\u001b[0m\u001b[44m advertis\u001b[0m\u001b[41mements\u001b[0m\u001b[44m and\u001b[0m\u001b[41m maps\u001b[0m\u001b[44m.\u001b[0m\u001b[41m \u001b[0m\u001b[44m\n",
      "\u001b[0m\u001b[41mif\u001b[0m\u001b[44m we\u001b[0m\u001b[41m can\u001b[0m\u001b[44m look\u001b[0m\u001b[41m at\u001b[0m\u001b[44m something\u001b[0m\u001b[41m with\u001b[0m\u001b[44m words\u001b[0m\u001b[41m and\u001b[0m\u001b[44m sent\u001b[0m\u001b[41mences\u001b[0m\u001b[44m,\u001b[0m\u001b[41m explore\u001b[0m\u001b[44m it\u001b[0m\u001b[41m,\u001b[0m\u001b[44m find\u001b[0m\u001b[41m layers\u001b[0m\u001b[44m of\u001b[0m\u001b[41m meaning\u001b[0m\u001b[44m in\u001b[0m\u001b[41m it\u001b[0m\u001b[44m,\u001b[0m\u001b[41m and\u001b[0m\u001b[44m draw\u001b[0m\u001b[41m information\u001b[0m\u001b[44m and\u001b[0m\u001b[41m con\u001b[0m\u001b[44mclus\u001b[0m\u001b[41mions\u001b[0m\u001b[44m from\u001b[0m\u001b[41m it\u001b[0m\u001b[44m,\u001b[0m\u001b[41m you\u001b[0m\u001b[44m’\u001b[0m\u001b[41mre\u001b[0m\u001b[44m looking\u001b[0m\u001b[41m at\u001b[0m\u001b[44m a\u001b[0m\u001b[41m text\u001b[0m\u001b[44m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from modules.tokenizer import Tokenizer\n",
    "\n",
    "print(\"cased tokenization:\")\n",
    "cased_tokenizer = Tokenizer.init_and_load(\"checkpoints/tokenizer/tokenizer_25k_10k.pkl\")\n",
    "cased_tokenizer.visualize(text)\n",
    "\n",
    "print(\"uncased tokenization:\")\n",
    "uncased_tokenizer = Tokenizer.init_and_load(\"checkpoints/tokenizer/tokenizer_15k_10k_uncased.pkl\")\n",
    "uncased_tokenizer.visualize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель\n",
    "\n",
    "Модель совпадает с трансформер-декодером из \"Attention is all you need\", за исключением того, что LayerNorm теперь находится перед слоями, а не после.\n",
    "\n",
    "Также используется weight sharing между слоем эмбеддингов и финальной проекцией.\n",
    "\n",
    "Используется инициализация линейных слоёв $\\sim N\\left(0, \\frac{0.02}{\\sqrt{2 \\cdot depth}}\\right)$, благодаря чему получаются одинаковые дисперсии активаций на разных слоях при инициализаии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: TransformerConfig(vocab_size=15256, d_model=768, context_length=512, n_heads=12, n_layers=12, p_dropout=0.1)\n",
      "params: 96,744,960\n"
     ]
    }
   ],
   "source": [
    "from modules.transformer import Transformer\n",
    "\n",
    "transformer = Transformer()\n",
    "\n",
    "print(f\"config: {transformer.config}\")\n",
    "print(f\"params: {sum(p.numel() for p in transformer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding_table): Embedding(15256, 768)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-11): 12 x MaskedSelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_projection): Linear(in_features=768, out_features=15256, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные\n",
    "\n",
    "Данные взяты из англоязычного дампа википедии. \n",
    "\n",
    "Предобрабатываются скриптом `scripts/preprocess_data.py`, где разделяются на train / validation, токенизируются, паддятся, и сохраняются чанками по 10000 айтемов.\n",
    "\n",
    "В интерфейсе доступа к данным реализовано кэширование этих чанков, что при последовательном доступе к данным минимизирует расходы на i/o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y', 'pad_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.data import WikipediaTokenizedDataset \n",
    "\n",
    "dataset = WikipediaTokenizedDataset(\"data/uncased-15k-10k/test\")\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение\n",
    "\n",
    "При обучении использовались следующий гиперпарметры:\n",
    "- lr = 6e-4\n",
    "- batch = 8\n",
    "- grad_accum = 2 -> 32 (увеличивался по мере обучения)\n",
    "- weight_decay = 0.1 для линейных слоёв\n",
    "\n",
    "Таким образом, количество токенов на одну оптимизацию было 8192 в начале и 131072 в конце\n",
    "\n",
    "Также использовался косинусный scheduler с линейным warmup на первые 10% шагов \n",
    "\n",
    "Всего было проведено 5000 шагов оптимизации (что, конечно, очень мало)\n",
    "\n",
    "Скрипт обучения: `scripts/train_transformer.py`\n",
    "\n",
    "Логи обучения: `train_log.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   97,  3055,   336,   257,  9857,   283,  5101,   384, 12112,   115,\n",
       "            257,   953,   283,  2679,   657,    46],\n",
       "         [   97,  3055,   614,   361,  1913,  6061,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer.init_and_load(\"checkpoints/transformer_uncased_5000_steps/ckpts/model_5000.pt\")\n",
    "\n",
    "prompts = [\n",
    "    \"a text is a passage of words that conveys a set of meanings.\",\n",
    "    \"A text can be written materials\",\n",
    "]\n",
    "\n",
    "inputs, pad_mask = uncased_tokenizer(prompts)\n",
    "inputs, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. that in 2010, a  references, ',\n",
       " 'a text can be written materials, to and to the former and he as an']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top_k = 50 by default\n",
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. =========',\n",
       " 'a text can be written materials, the the the the the the the the the']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, greedy=True)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. she. the the same she when ==',\n",
       " 'a text can be written materials-american a also he of the been the the']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, top_k=50)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"a text is a passage of words that conveys a set of meanings. or well in congo's =13 \",\n",
       " 'a text can be written materials single their average was australian the she eastern toronto too']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, top_p=0.9, top_k=None)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. in the the the the the the the the the',\n",
       " 'a text can be written materials. in the a the an the the a the']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low\n",
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, temperature=0.5)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. as most would time that a not the this were',\n",
       " 'a text can be written materials. was it the this by an 12 at']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# high\n",
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, temperature=2.5)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repetition penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. a a a a a a a a a a',\n",
       " 'a text can be written materialsaaaaaaaaaa']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low\n",
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, repetition_penalty=0.1)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a text is a passage of words that conveys a set of meanings. the ( was born he from =–',\n",
       " \"a text can be written materials's a known. he who as film of\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# high\n",
    "generated_tokens = transformer.generate(inputs, 10, attn_mask=pad_mask, repetition_penalty=2.0)\n",
    "uncased_tokenizer.decode_batch(generated_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
